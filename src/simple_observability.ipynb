{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6a2adc9-23e4-458a-b554-cb687a888ed8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Simple Observability \n",
    "\n",
    "_Created by Eumar Assis (eumar.assis@databricks.com_)\n",
    "\n",
    "This notebook computes and consolidates observability metrics for the Lakehouse into an unified Delta table, making it easy to build operational dashboards. It provides the following insights:\n",
    "\n",
    "- Inventory of all tables, including their format and storage location\n",
    "- Table size and growth over time\n",
    "- Data Freshness: tables change dates\n",
    "- Last optimization and vacuum dates\n",
    "\n",
    "\n",
    "\n",
    "**How does it work?** The notebook queries the [system.information_schema](https://docs.databricks.com/en/sql/language-manual/sql-ref-information-schema.html) table based on the provided regular expression to find the desired tables. Then, it uses [DESCRIBE DETAIL](https://docs.databricks.com/en/delta/table-details.html) and [DESCRIBE HISTORY](https://docs.databricks.com/en/sql/language-manual/delta-describe-history.html) for each table to collect the appropriate metrics. The notebook uses a temporary table to make the code idempotent. It will only replace the records in the destination table at the end with an atomic MERGE.\n",
    "\n",
    "Based on initial tests, the notebook script was able to process 314 tables in 40 minutes by using a 3-nodes cluster with 32GB nodes. Choosing a larger master node and more workers is likely to improve the performance and run the script faster.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "- User running this notebook will need the USE CATALOG, USE SCHEMA, and SELECT permissions on tables targeted for the script.\n",
    "\n",
    "**Parameters:**\n",
    "\n",
    "- **destination_table**: provide the name of the destination table, must use a three-level namespace. e.g. catalog_name.schema_name.table_name\n",
    "- **filter**: regular expression to filter which catalogs, schemas, or tables the metrics should be collected for. Examples:\n",
    "  - All schemas and tables under catalog 'demo': `^demo\\.`\n",
    "  - All tables under catalog 'demo' and schema 'dbdemo': `^demo\\.dbdemo\\.`\n",
    "  - Single table 'tabledemo': `demo.dbdemo.tabledemo`\n",
    "\n",
    "**Output Table:**\n",
    "\n",
    "\n",
    "| Column Name             | Type       | Description                            |\n",
    "|-------------------------|------------|----------------------------------------|\n",
    "| `timestamp`             | TIMESTAMP  | The timestamp of the record            |\n",
    "| `catalogName`           | STRING     | The name of the catalog                |\n",
    "| `schemaName`            | STRING     | The name of the schema                 |\n",
    "| `tableName`             | STRING     | The name of the table                  |\n",
    "| `creationDate`          | TIMESTAMP  | The creation date of the table         |\n",
    "| `createdBy`             | STRING     | The creator of the table               |\n",
    "| `owner`                 | STRING     | The owner of the table                 |\n",
    "| `type`                  | STRING     | The type of the table                  |\n",
    "| `format`                | STRING     | The format of the table                |\n",
    "| `storageLocation`       | STRING     | The storage location of the table      |\n",
    "| `lastVacuumDate`        | TIMESTAMP  | The date of the last vacuum operation  |\n",
    "| `lastOptimizeDate`      | TIMESTAMP  | The date of the last optimize operation|\n",
    "| `lastSchemaChangeDate`  | TIMESTAMP  | The date of the last schema change     |\n",
    "| `lastWriteDate`         | TIMESTAMP  | The date of the last write operation   |\n",
    "| `sizeInMB`              | DOUBLE     | The size of the table in MB            |\n",
    "| `sizeInMB24Hours`       | DOUBLE     | The size of the table in the last 24 hours |\n",
    "| `sizeInMB7Days`         | DOUBLE     | The size of the table in the last 7 days   |\n",
    "| `sizeInMb30Days`        | DOUBLE     | The size of the table in the last 30 days  |\n",
    "| `partitionColumns`      | STRING     | The partition columns of the table     |\n",
    "| `clusteringColumns`     | STRING     | The clustering columns of the table    |\n",
    "| `sizeHistory`           | STRING     | This size history, storing a JSON array     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19dece98-6bbf-4e01-8e5f-4c3d824c1e22",
     "showTitle": true,
     "title": "Declare parameter widgets"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dbutils.widgets.text(\"filter\", \"^catalogName\\.\", \"Table Filter Regex\")\n",
    "\n",
    "dbutils.widgets.text(\"destination_table\", \"eumar_tests.eumar_default.simple_observability\", \"Destination table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3784558d-4b44-4aad-93b1-08899b2c6aaf",
     "showTitle": true,
     "title": "Create destination table based on widget parameter"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Get the destination table name from the widget\n",
    "destination_table = dbutils.widgets.get(\"destination_table\")\n",
    "\n",
    "temporary_table = destination_table + \"_temp_processing_\" + datetime.now().strftime('%Y%m%d%H')\n",
    "\n",
    "# Creates the destination Delta table\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {destination_table}\n",
    "(\n",
    "  timestamp TIMESTAMP,\n",
    "  catalogName STRING,\n",
    "  schemaName STRING,\n",
    "  tableName STRING,\n",
    "  creationDate TIMESTAMP,\n",
    "  createdBy STRING,\n",
    "  owner STRING,\n",
    "  type STRING,\n",
    "  format STRING,\n",
    "  storageLocation STRING,\n",
    "  lastVacuumDate TIMESTAMP,\n",
    "  lastOptimizeDate TIMESTAMP,\n",
    "  lastSchemaChangeDate TIMESTAMP,\n",
    "  lastWriteDate TIMESTAMP,\n",
    "  sizeInMB DOUBLE,\n",
    "  sizeInMB24Hours DOUBLE,\n",
    "  sizeInMB7Days DOUBLE,\n",
    "  sizeInMb30Days DOUBLE,\n",
    "  partitionColumns STRING,\n",
    "  clusteringColumns STRING,\n",
    "  sizeHistory STRING  -- This column will store the JSON data\n",
    ")\n",
    "USING delta\n",
    "\"\"\")\n",
    "\n",
    "# Create a temporary table with the same schema as the original table\n",
    "spark.sql(f\"SELECT * FROM {destination_table} LIMIT 0\").write.mode(\"overwrite\").saveAsTable(temporary_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cf8374e-6b89-4c76-ad1e-8abacd6c8dff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "To find the targeted tables, we will query and filter the system.information_schema table using the Regex. Then, we'll insert the target tables into the temporary table.\n",
    "https://docs.databricks.com/en/sql/language-manual/sql-ref-information-schema.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "258740dd-7049-4f49-aaec-2243e0b5561e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select distinct table_type from system.information_schema.tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dd232ff-dc42-4c63-95b1-a2b862530d16",
     "showTitle": true,
     "title": "Find all target tables from information_schema and insert to temp table"
    }
   },
   "outputs": [],
   "source": [
    "# Get the value of the widget\n",
    "table_filter_regex = dbutils.widgets.get(\"filter\")\n",
    "\n",
    "# Define query to insert filtered results into the temp table\n",
    "sql_query = f\"\"\"\n",
    "INSERT INTO {temporary_table} \n",
    "  (\n",
    "    timestamp,\n",
    "    catalogName,\n",
    "    schemaName,\n",
    "    tableName,\n",
    "    creationDate,\n",
    "    createdBy,\n",
    "    owner,\n",
    "    lastSchemaChangeDate,    \n",
    "    type,\n",
    "    format,\n",
    "    sizeHistory\n",
    "  )\n",
    "SELECT\n",
    "  CURRENT_DATE(),\n",
    "  table_catalog,\n",
    "  table_schema,\n",
    "  table_name,\n",
    "  created,\n",
    "  created_by,  \n",
    "  table_owner,\n",
    "  last_altered,\n",
    "  table_type,\n",
    "  data_source_format,\n",
    "  to_json(array())\n",
    "\n",
    "FROM system.information_schema.tables\n",
    "WHERE CONCAT(table_catalog, '.', table_schema, '.', table_name) RLIKE r'{table_filter_regex}'\n",
    "AND table_owner != 'System user' AND table_type IN ('MANAGED', 'EXTERNAL' )\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL query\n",
    "spark.sql(sql_query)\n",
    "\n",
    "display(spark.sql(f\"SELECT COUNT(*) as records_to_process FROM {temporary_table}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c42b44c-f3e6-4479-b67e-a35e278f8333",
     "showTitle": true,
     "title": "Declare function to get history"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col\n",
    "import json\n",
    "\n",
    "# Define a function to filter and get the first timestamp for a given set of operations\n",
    "def get_first_timestamp(operations, history_array):\n",
    "    filtered = [row[\"timestamp\"] for row in history_array if row[\"operation\"] in operations]\n",
    "    return filtered[0] if filtered else None\n",
    "  \n",
    "# this cannot be an UDF since UDFs cannot access the spark context\n",
    "def get_update_history(catalog, schema, table, table_format):\n",
    "\n",
    "  details = spark.sql(f\"DESCRIBE DETAIL `{catalog}`.`{schema}`.`{table}`\").collect()[0]\n",
    "\n",
    "  last_vacuum_date = None\n",
    "  last_optimize_date = None\n",
    "  last_write_date = None\n",
    "\n",
    "  if table_format == \"DELTA\":\n",
    "    history = spark.sql(f\"DESCRIBE HISTORY `{catalog}`.`{schema}`.`{table}`\").orderBy(\"timestamp\", ascending=False).collect()\n",
    "    \n",
    "    #Extract the required columns\n",
    "    # vacuum_date_row = history.filter(col(\"operation\").isin([\"VACUUM\"])).select(\"timestamp\").first()\n",
    "    # optimize_date_row = history.filter(col(\"operation\").isin([\"OPTIMIZE\"])).select(\"timestamp\").first()\n",
    "    # last_write_date_row = history.filter(col(\"operation\").isin([\"WRITE\", \"MERGE\", \"DELETE\", \"UPDATE\"])).select(\"timestamp\").first()\n",
    "\n",
    "    last_vacuum_date = get_first_timestamp([\"VACUUM\"], history)\n",
    "    last_optimize_date = get_first_timestamp([\"OPTIMIZE\"], history)\n",
    "    last_write_date = get_first_timestamp([\"WRITE\", \"MERGE\", \"DELETE\", \"UPDATE\"], history)\n",
    "\n",
    "  size_in_mb = 0 if details[\"sizeInBytes\"] is None else details[\"sizeInBytes\"] / (1024 * 1024)\n",
    "  partition_columns = \",\".join(details[\"partitionColumns\"]) if details[\"partitionColumns\"] is not None else \"\"\n",
    "  clustering_columns = \",\".join(details[\"clusteringColumns\"]) if details[\"clusteringColumns\"] is not None else \"\"\n",
    "  storage_location = details[\"location\"]\n",
    "\n",
    "\n",
    "  return {\n",
    "        \"lastVacuumDate\": last_vacuum_date,\n",
    "        \"lastOptimizeDate\": last_optimize_date,\n",
    "        \"lastWriteDate\": last_write_date,\n",
    "        \"sizeInMB\": float(size_in_mb),\n",
    "        \"storageLocation\": storage_location,\n",
    "        \"partitionColumns\": partition_columns,\n",
    "        \"clusteringColumns\": clustering_columns,\n",
    "        \"sizeHistory\" : json.dumps([{\n",
    "          \"size\" : float(size_in_mb),\n",
    "          \"date\" : datetime.now().strftime('%Y-%m-%d')\n",
    "        }])\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9feb01fb-46db-4396-b569-f0162bc524c2",
     "showTitle": true,
     "title": "Get update history for each record and update in bulk using thread pool"
    }
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "# Get the DataFrame\n",
    "df = spark.sql(f\"SELECT * FROM {temporary_table}\")\n",
    "\n",
    "# Create a new list to store the updated rows\n",
    "updated_rows = []\n",
    "\n",
    "rows_processed = 0\n",
    "\n",
    "rows_collect = df.collect()\n",
    "\n",
    "print(f\"Getting history for tables. Total tables: {len(rows_collect)}\")\n",
    "\n",
    "# Define a function to process each row and update the columns\n",
    "def process_row(row):\n",
    "    try:\n",
    "        update_history = get_update_history(row['catalogName'], row['schemaName'], row['tableName'], row['format'])\n",
    "\n",
    "        updated_row = row.asDict()\n",
    "\n",
    "        for key in update_history.keys():\n",
    "            updated_row[key] = update_history[key]\n",
    "\n",
    "        return updated_row\n",
    "\n",
    "    except Exception as e:\n",
    "        # Code to handle any other exceptions\n",
    "        print(f\"{row['catalogName']}.{row['schemaName']}.{row['tableName']} failed\")\n",
    "        print(\"An error occurred:\", str(e))\n",
    "\n",
    "# Create a ThreadPoolExecutor with a maximum of 5 threads\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    # Submit each row to the executor for processing\n",
    "    futures = [executor.submit(process_row, row) for row in rows_collect]\n",
    "\n",
    "    # Iterate over the completed futures and get the updated rows\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        updated_row = future.result()\n",
    "        if updated_row:\n",
    "            updated_rows.append(updated_row)\n",
    "            rows_processed += 1\n",
    "            print(f\"({rows_processed}) {updated_row['catalogName']}.{updated_row['schemaName']}.{updated_row['tableName']} completed\")\n",
    "\n",
    "# Create a new DataFrame with the updated rows\n",
    "updated_df = spark.createDataFrame(updated_rows, df.schema)\n",
    "\n",
    "# Save the changes in bulk\n",
    "updated_df.write.mode(\"overwrite\").insertInto(temporary_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d48c8688-bd8f-478c-a734-84cd7eb35be2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sql_query = f\"\"\"\n",
    "MERGE INTO {destination_table} AS d\n",
    "USING {temporary_table} AS i\n",
    "ON d.catalogName = i.catalogName\n",
    "   AND d.schemaName = i.schemaName\n",
    "   AND d.tableName = i.tableName\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET\n",
    "    d.timestamp = CURRENT_DATE(),\n",
    "    d.creationDate = i.creationDate,\n",
    "    d.createdBy = i.createdBy,\n",
    "    d.lastSchemaChangeDate = i.lastSchemaChangeDate,\n",
    "    d.owner = i.owner,\n",
    "    d.type = i.type,\n",
    "    d.format = i.format,\n",
    "    d.sizeInMB = i.sizeInMB,\n",
    "    d.storageLocation = i.storageLocation,\n",
    "    d.lastVacuumDate = i.lastVacuumDate,\n",
    "    d.lastOptimizeDate = i.lastOptimizeDate,\n",
    "    d.lastWriteDate = i.lastWriteDate,\n",
    "    d.partitionColumns = i.partitionColumns,\n",
    "    d.clusteringColumns = i.clusteringColumns,\n",
    "    d.sizeHistory = to_json(array_union(from_json(d.sizeHistory, 'array<struct<date:string,size:double>>'),\n",
    "                                        from_json(i.sizeHistory, 'array<struct<date:string,size:double>>')))\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (\n",
    "    timestamp,\n",
    "    catalogName,\n",
    "    schemaName,\n",
    "    tableName,\n",
    "    creationDate,\n",
    "    createdBy,\n",
    "    lastSchemaChangeDate,\n",
    "    owner,\n",
    "    type,\n",
    "    format,\n",
    "    sizeInMB,\n",
    "    storageLocation,\n",
    "    lastVacuumDate,\n",
    "    lastOptimizeDate,\n",
    "    lastWriteDate,\n",
    "    partitionColumns,\n",
    "    clusteringColumns,    \n",
    "    sizeHistory\n",
    "  )\n",
    "  VALUES (\n",
    "    CURRENT_DATE(),\n",
    "    i.catalogName,\n",
    "    i.schemaName,\n",
    "    i.tableName,\n",
    "    i.creationDate,\n",
    "    i.createdBy,\n",
    "    i.lastSchemaChangeDate,\n",
    "    i.owner,\n",
    "    i.type,\n",
    "    i.format,\n",
    "    i.sizeInMB,\n",
    "    i.storageLocation,\n",
    "    i.lastVacuumDate,\n",
    "    i.lastOptimizeDate,\n",
    "    i.lastWriteDate,\n",
    "    i.partitionColumns,\n",
    "    i.clusteringColumns,    \n",
    "    i.sizeHistory\n",
    "  )\n",
    "\"\"\"\n",
    "\n",
    "display(spark.sql(sql_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6032765c-5a86-46eb-ab4c-7a4a593d8152",
     "showTitle": true,
     "title": "compute size growth using explode on sizeHistory"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "sql_query = f\"\"\"\n",
    "MERGE INTO {destination_table} AS target\n",
    "USING (\n",
    "  WITH exploded_table AS (\n",
    "      SELECT \n",
    "          t.catalogName,\n",
    "          t.schemaName,\n",
    "          t.tableName,\n",
    "          explode(from_json(t.sizeHistory, 'array<struct<date:string, size:double>>')) AS sizeHistoryElement\n",
    "      FROM \n",
    "          {destination_table} t\n",
    "      INNER JOIN\n",
    "        {temporary_table} s\n",
    "      ON t.catalogName = s.catalogName AND t.schemaName = s.schemaName AND t.tableName = s.tableName\n",
    "      ORDER BY sizeHistoryElement.date DESC          \n",
    "  )\n",
    "  SELECT  \n",
    "      catalogName,\n",
    "      schemaName,\n",
    "      tableName,\n",
    "      MAX(CASE WHEN date_diff(current_date(), sizeHistoryElement.date) >= 1 THEN sizeHistoryElement.size ELSE NULL END) AS sizeInMB24Hours,\n",
    "      MAX(CASE WHEN date_diff(current_date(), sizeHistoryElement.date) >= 7 THEN sizeHistoryElement.size ELSE NULL END) AS sizeInMB7Days,\n",
    "      MAX(CASE WHEN date_diff(current_date(), sizeHistoryElement.date) >= 30 THEN sizeHistoryElement.size ELSE NULL END) AS sizeInMB30Days\n",
    "  FROM \n",
    "      exploded_table\n",
    "  GROUP BY\n",
    "      catalogName,\n",
    "      schemaName,\n",
    "      tableName\n",
    ") AS source\n",
    "ON target.catalogName = source.catalogName AND target.schemaName = source.schemaName AND target.tableName = source.tableName\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET\n",
    "    target.sizeInMB24Hours = source.sizeInMB24Hours,\n",
    "    target.sizeInMB7Days = source.sizeInMB7Days,\n",
    "    target.sizeInMb30Days = source.sizeInMb30Days;\n",
    "\"\"\"\n",
    "\n",
    "display(spark.sql(sql_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d51bb7f-bec1-4e8b-b5a0-41eaf588f6d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "SELECT \n",
    "  t.* \n",
    "FROM \n",
    "  {destination_table} t\n",
    "INNER JOIN\n",
    "  {temporary_table} s\n",
    "  ON t.catalogName = s.catalogName AND t.schemaName = s.schemaName AND t.tableName = s.tableName \n",
    "\"\"\"\n",
    "\n",
    "display(spark.sql(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c10f7b61-f5e9-466b-8965-7c5aa6f05458",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP TABLE {temporary_table}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1935579448337091,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2,
    "widgetLayout": [
     {
      "breakBefore": false,
      "name": "destination_table",
      "width": 180
     },
     {
      "breakBefore": false,
      "name": "filter",
      "width": 180
     }
    ]
   },
   "notebookName": "simple_observability",
   "widgets": {
    "destination_table": {
     "currentValue": "eumar_tests.eumar_default.simple_observability",
     "nuid": "11910578-4e60-4062-86a4-32c014223067",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "defaultValue": "eumar_tests.eumar_default.simple_observability",
      "label": "Destination table",
      "name": "destination_table",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "filter": {
     "currentValue": "^dbdemos\\.",
     "nuid": "9687348d-eead-4b50-b93c-13927c0e3a84",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "defaultValue": "^catalogName\\.",
      "label": "Table Filter Regex",
      "name": "filter",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
